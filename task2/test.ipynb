{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI608 Task 2 Recommendation network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import and define some important function and repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji # pip install emoji\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sklearn\n",
    "\n",
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import KFold\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_emoji_to_word_dict(num_line=1000, filename='emojitweets-01-04-2018.txt', stopword_filename=\"english.txt\", threshold=2):\n",
    "    '''\n",
    "    This function helps you to get the raw dict format: {emoji:{word:weight}}\n",
    "    :param num_line: how many lines are used, this is implemented by a unix-like command line\n",
    "    :param filename: the filename of the raw data\n",
    "    :param stopword_filename: a stopword list, which will be provided\n",
    "    :param threshold: if `weight` in {emoji:{word:weight}} is smaller than `threshold`, then the word is abandoned.\n",
    "    '''\n",
    "    time_start = time.time()\n",
    "    cmdline = \"head -n {} {} > tmp_{}.txt\".format(num_line, filename, num_line)\n",
    "    print(\"processing:\", cmdline)\n",
    "    os.system(cmdline)\n",
    "    print(\"complete, {} second is used\".format(time.time()-time_start))\n",
    "    \n",
    "    time_start = time.time()\n",
    "    print(\"generating stopword from\", stopword_filename)\n",
    "    stopword = set()\n",
    "    fs = open(stopword_filename, 'r', encoding='utf-8')\n",
    "    line = fs.readline()\n",
    "    while line:\n",
    "        stopword.add(line.strip())\n",
    "        line = fs.readline()\n",
    "    fs.close()\n",
    "    print(\"complete, {} second is used\".format(time.time()-time_start))\n",
    "    \n",
    "    time_start = time.time()\n",
    "    print(\"generating dict from\", \"tmp_{}.txt\".format(num_line))\n",
    "    f = open(\"tmp_{}.txt\".format(num_line), 'r', encoding='utf-8')\n",
    "    line = f.readline()\n",
    "    sl = {}\n",
    "    while line:\n",
    "        line = line.strip()\n",
    "        line = emoji.demojize(line) \n",
    "        emoji_list = re.findall(r\":[\\w-]*:\",line)\n",
    "        emoji_set = set(emoji_list)\n",
    "        for emoji_item in emoji_set:\n",
    "            line = line.replace(emoji_item, \"\")\n",
    "        word_set = set()\n",
    "        for word in line.lower().split():\n",
    "            if (not word in stopword) and (not word in string.punctuation):\n",
    "                word_set.add(word)\n",
    "        for emoji_item in emoji_set:\n",
    "            if emoji_item in sl:\n",
    "                for word in word_set:\n",
    "                    if word in sl[emoji_item]:\n",
    "                        sl[emoji_item][word] += 1\n",
    "                    else:\n",
    "                        sl[emoji_item][word] = 1\n",
    "            else:\n",
    "                sl[emoji_item] = {}\n",
    "                for word in word_set:\n",
    "                    if word in sl[emoji_item]:\n",
    "                        sl[emoji_item][word] += 1\n",
    "                    else:\n",
    "                        sl[emoji_item][word] = 1\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "    print(\"complete, {} second is used\".format(time.time()-time_start))\n",
    "    \n",
    "    time_start = time.time()\n",
    "    print(\"filtering dict\")\n",
    "    sl_tmp = {}\n",
    "    for key in sl.keys():\n",
    "        sl_tmp[key] = {}\n",
    "        for word in sl[key].keys():\n",
    "            if sl[key][word] >= threshold:\n",
    "                sl_tmp[key][word] = sl[key][word]\n",
    "        if len(sl_tmp[key]) == 0:\n",
    "            del sl_tmp[key]\n",
    "    print(\"complete, {} second is used\".format(time.time()-time_start))\n",
    "    \n",
    "    return sl_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part you need to generate a dict at format {emoji:{word:weight}}, if you use the following param, the typical time for processing is 270 seconds on i7-9750H."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: head -n 1000000 emojitweets-01-04-2018.txt > tmp_1000000.txt\n",
      "complete, 0.2802700996398926 second is used\n",
      "generating stopword from english.txt\n",
      "complete, 0.0009958744049072266 second is used\n",
      "generating dict from tmp_1000000.txt\n",
      "complete, 264.52676796913147 second is used\n",
      "filtering dict\n",
      "complete, 0.3306615352630615 second is used\n"
     ]
    }
   ],
   "source": [
    "sl = gen_emoji_to_word_dict(num_line=1000000, threshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore a little bit about the dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('followers', 269), ('…', 256), ('grow', 206), ('&amp;', 201), (':)', 201)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sl[':rocket:'].items(), key=lambda x:-x[1])[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save it and load it next time for saving your time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = open(\"myfile.json\", \"w\") \n",
    "json.dump(sl, out_file) \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"myfile.json\", \"r\", encoding='utf-8')\n",
    "sl = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('followers', 269), ('…', 256), ('grow', 206), ('&amp;', 201), (':)', 201)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sl[':rocket:'].items(), key=lambda x:-x[1])[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we need to extract a word corpus, and find out its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_set = {}\n",
    "for key in sl.keys():\n",
    "    for word in sl[key].keys():\n",
    "        if word not in all_word_set:\n",
    "            all_word_set[word] = 1\n",
    "        else:\n",
    "            all_word_set[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19217"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to set a filter here, please change the filter number below. It is 1 by default which means all word are accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: !!! This filter_num is not used in the code following, you should set it to 1 at least for now to make the following code runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19217"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word_list = list(filter(lambda x:x[1]>=filter_num, all_word_set.items()))\n",
    "len(new_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('game', 164), ('2', 275), ('rating:', 3), ('gave', 52), ('5/5', 4)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate a map from emoji/word to its unique id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word_list = list(map(lambda x:x[0], new_word_list))\n",
    "new_emoji_list = list(sl.keys())\n",
    "new_word_set = set(new_word_list)\n",
    "emoji_enu = list(enumerate(new_emoji_list))\n",
    "word_enu = list(enumerate(new_word_list))\n",
    "map_emoji = dict(emoji_enu)\n",
    "map_word = dict(word_enu)\n",
    "map_emoji=dict(zip(map_emoji.values(),map_emoji.keys()))\n",
    "map_word=dict(zip(map_word.values(),map_word.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_emoji[\":rocket:\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_word[\"history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are two kinds of rate calculation method, the first is maxmin scale, the second is uniform distribution scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = open('all_rec.csv','w',newline='')\n",
    "csv_write = csv.writer(out,dialect='excel')\n",
    "header = [\"word_id\", \"emoji_id\", \"weight\"]\n",
    "csv_write.writerow(header)\n",
    "for key in sl.keys():\n",
    "    maxval = max(sl[key].values())\n",
    "    minval = min(sl[key].values())\n",
    "    for word in sl[key].keys() :\n",
    "        item = [map_word[word], map_emoji[key], int((sl[key][word]-minval)/(maxval-minval+0.01)*10)+1]\n",
    "        csv_write.writerow(item)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = open('all_rec.csv','w',newline='')\n",
    "csv_write = csv.writer(out,dialect='excel')\n",
    "header = [\"word_id\", \"emoji_id\", \"weight\"]\n",
    "csv_write.writerow(header)\n",
    "for key in sl.keys():\n",
    "    val_list = list(sl[key].values())\n",
    "    val_list.sort()\n",
    "    length = len(val_list)\n",
    "    for word in sl[key].keys():\n",
    "        item = [map_word[word], map_emoji[key], int(val_list.index(sl[key][word])/length*10)+1]\n",
    "        csv_write.writerow(item)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the csv you just dumped. and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df = pd.read_csv(\"all_rec.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(rec_df.weight.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(rec_df.weight.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df = sklearn.utils.shuffle(rec_df,random_state=671)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>emoji_id</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83264</th>\n",
       "      <td>21</td>\n",
       "      <td>228</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106508</th>\n",
       "      <td>4939</td>\n",
       "      <td>492</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29304</th>\n",
       "      <td>11633</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66177</th>\n",
       "      <td>2802</td>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110933</th>\n",
       "      <td>16415</td>\n",
       "      <td>595</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32408</th>\n",
       "      <td>2370</td>\n",
       "      <td>37</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105135</th>\n",
       "      <td>417</td>\n",
       "      <td>475</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83679</th>\n",
       "      <td>4657</td>\n",
       "      <td>228</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4385</th>\n",
       "      <td>2470</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71347</th>\n",
       "      <td>778</td>\n",
       "      <td>146</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122154 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word_id  emoji_id  weight\n",
       "83264        21       228       6\n",
       "106508     4939       492       5\n",
       "29304     11633        33       6\n",
       "66177      2802       132       4\n",
       "110933    16415       595       2\n",
       "...         ...       ...     ...\n",
       "32408      2370        37       8\n",
       "105135      417       475       5\n",
       "83679      4657       228      10\n",
       "4385       2470        11       7\n",
       "71347       778       146      10\n",
       "\n",
       "[122154 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply some baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3164549156315317\n",
      "2.3089295396279628\n",
      "2.316460608671727\n"
     ]
    }
   ],
   "source": [
    "reader = Reader(rating_scale=(1,10))\n",
    "data = Dataset.load_from_df(rec_df[['emoji_id', 'word_id', 'weight']], reader)\n",
    "kf = KFold(n_splits=3)\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo = SVD(n_epochs=100)\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    print(accuracy.mae(predictions, verbose=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
