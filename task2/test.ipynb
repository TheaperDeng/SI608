{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI608 Task 2 Recommendation network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import and define some important function and repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji # pip install emoji\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sklearn\n",
    "\n",
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import KFold\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_emoji_to_word_dict(num_line=1000, filename='emojitweets-01-04-2018.txt', stopword_filename=\"english.txt\", threshold=2):\n",
    "    '''\n",
    "    This function helps you to get the raw dict format: {emoji:{word:weight}}\n",
    "    :param num_line: how many lines are used, this is implemented by a unix-like command line\n",
    "    :param filename: the filename of the raw data\n",
    "    :param stopword_filename: a stopword list, which will be provided\n",
    "    :param threshold: if `weight` in {emoji:{word:weight}} is smaller than `threshold`, then the word is abandoned.\n",
    "    '''\n",
    "    time_start = time.time()\n",
    "    \n",
    "    # For windows OS\n",
    "    # Note that type command on windows does not receive -n as parameter\n",
    "    # Extract first lines before running the program \n",
    "    cmdline = \"type -n {} {} > tmp_{}.txt\".format(num_line, filename, num_line)\n",
    "    \n",
    "    # For Unix OS\n",
    "    # cmdline = \"head -n {} {} > tmp_{}.txt\".format(num_line, filename, num_line)\n",
    "    print(\"processing:\", cmdline)\n",
    "    os.system(cmdline)\n",
    "    print(\"complete, {} second is used\".format(time.time()-time_start))\n",
    "    \n",
    "    time_start = time.time()\n",
    "    print(\"generating stopword from\", stopword_filename)\n",
    "    stopword = set()\n",
    "    fs = open(stopword_filename, 'r', encoding='utf-8')\n",
    "    line = fs.readline()\n",
    "    while line:\n",
    "        stopword.add(line.strip())\n",
    "        line = fs.readline()\n",
    "    fs.close()\n",
    "    print(\"complete, {} second is used\".format(time.time()-time_start))\n",
    "    \n",
    "    time_start = time.time()\n",
    "    print(\"generating dict from\", \"tmp_{}.txt\".format(num_line))\n",
    "    f = open(\"tmp_{}.txt\".format(num_line), 'r', encoding='utf-8')\n",
    "    line = f.readline()\n",
    "    sl = {}\n",
    "    while line:\n",
    "        line = line.strip()\n",
    "        line = emoji.demojize(line) \n",
    "        emoji_list = re.findall(r\":[\\w-]*:\",line)\n",
    "        emoji_set = set(emoji_list)\n",
    "        for emoji_item in emoji_set:\n",
    "            line = line.replace(emoji_item, \"\")\n",
    "        word_set = set()\n",
    "        for word in line.lower().split():\n",
    "            if (not word in stopword) and (not word in string.punctuation):\n",
    "                word_set.add(word)\n",
    "        for emoji_item in emoji_set:\n",
    "            if emoji_item in sl:\n",
    "                for word in word_set:\n",
    "                    if word in sl[emoji_item]:\n",
    "                        sl[emoji_item][word] += 1\n",
    "                    else:\n",
    "                        sl[emoji_item][word] = 1\n",
    "            else:\n",
    "                sl[emoji_item] = {}\n",
    "                for word in word_set:\n",
    "                    if word in sl[emoji_item]:\n",
    "                        sl[emoji_item][word] += 1\n",
    "                    else:\n",
    "                        sl[emoji_item][word] = 1\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "    print(\"complete, {} second is used\".format(time.time()-time_start))\n",
    "    \n",
    "    time_start = time.time()\n",
    "    print(\"filtering dict\")\n",
    "    sl_tmp = {}\n",
    "    for key in sl.keys():\n",
    "        sl_tmp[key] = {}\n",
    "        for word in sl[key].keys():\n",
    "            if sl[key][word] >= threshold:\n",
    "                sl_tmp[key][word] = sl[key][word]\n",
    "        if len(sl_tmp[key]) == 0:\n",
    "            del sl_tmp[key]\n",
    "    print(\"complete, {} second is used\".format(time.time()-time_start))\n",
    "    \n",
    "    return sl_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part you need to generate a dict at format {emoji:{word:weight}}, if you use the following param, the typical time for processing is 270 seconds on i7-9750H."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: type -n 1000000 emojitweets-01-04-2018.txt > tmp_1000000.txt\n",
      "complete, 2.287729263305664 second is used\n",
      "generating stopword from english.txt\n",
      "complete, 0.0008327960968017578 second is used\n",
      "generating dict from tmp_1000000.txt\n",
      "complete, 345.1900963783264 second is used\n",
      "filtering dict\n",
      "complete, 0.3267679214477539 second is used\n"
     ]
    }
   ],
   "source": [
    "sl = gen_emoji_to_word_dict(num_line=1000000, threshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore a little bit about the dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('followers', 269), ('…', 256), ('grow', 206), ('&amp;', 201), (':)', 201)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sl[':rocket:'].items(), key=lambda x:-x[1])[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save it and load it next time for saving your time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = open(\"myfile.json\", \"w\") \n",
    "json.dump(sl, out_file) \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"myfile.json\", \"r\", encoding='utf-8')\n",
    "sl = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('followers', 269), ('…', 256), ('grow', 206), ('&amp;', 201), (':)', 201)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sl[':rocket:'].items(), key=lambda x:-x[1])[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we need to extract a word corpus, and find out its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_set = {}\n",
    "for key in sl.keys():\n",
    "    for word in sl[key].keys():\n",
    "        if word not in all_word_set:\n",
    "            all_word_set[word] = 1\n",
    "        else:\n",
    "            all_word_set[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19219"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to set a filter here, please change the filter number below. It is 1 by default which means all word are accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: !!! This filter_num is not used in the code following, you should set it to 1 at least for now to make the following code runnable.\n",
    "\n",
    "When filter_num is set to be bigger than 1, there are bugs in later parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19219"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word_list = list(filter(lambda x:x[1]>=filter_num, all_word_set.items()))\n",
    "len(new_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('game', 164), ('2', 275), ('rating:', 3), ('vote', 141), ('5/5', 4)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate a map from emoji/word to its unique id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word_list = list(map(lambda x:x[0], new_word_list))\n",
    "new_emoji_list = list(sl.keys())\n",
    "new_word_set = set(new_word_list)\n",
    "emoji_enu = list(enumerate(new_emoji_list))\n",
    "word_enu = list(enumerate(new_word_list))\n",
    "map_emoji = dict(emoji_enu)\n",
    "map_word = dict(word_enu)\n",
    "map_emoji=dict(zip(map_emoji.values(),map_emoji.keys()))\n",
    "map_word=dict(zip(map_word.values(),map_word.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_emoji[\":rocket:\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_word[\"history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are two kinds of rate calculation method, the first is maxmin scale, the second is uniform distribution scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = open('all_rec.csv','w',newline='')\n",
    "csv_write = csv.writer(out,dialect='excel')\n",
    "header = [\"word_id\", \"emoji_id\", \"weight\"]\n",
    "csv_write.writerow(header)\n",
    "for key in sl.keys():\n",
    "    maxval = max(sl[key].values())\n",
    "    minval = min(sl[key].values())\n",
    "    for word in sl[key].keys() :\n",
    "        item = [map_word[word], map_emoji[key], int((sl[key][word]-minval)/(maxval-minval+0.01)*10)+1]\n",
    "        csv_write.writerow(item)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = open('all_rec.csv','w',newline='')\n",
    "csv_write = csv.writer(out,dialect='excel')\n",
    "header = [\"word_id\", \"emoji_id\", \"weight\"]\n",
    "csv_write.writerow(header)\n",
    "for key in sl.keys():\n",
    "    val_list = list(sl[key].values())\n",
    "    val_list.sort()\n",
    "    length = len(val_list)\n",
    "    for word in sl[key].keys():\n",
    "        item = [map_word[word], map_emoji[key], int(val_list.index(sl[key][word])/length*10)+1]\n",
    "        csv_write.writerow(item)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the csv you just dumped. and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df = pd.read_csv(\"all_rec.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(rec_df.weight.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(rec_df.weight.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df = sklearn.utils.shuffle(rec_df,random_state=671)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>emoji_id</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57744</th>\n",
       "      <td>492</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20885</th>\n",
       "      <td>10099</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41872</th>\n",
       "      <td>1671</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122006</th>\n",
       "      <td>14235</td>\n",
       "      <td>1297</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40583</th>\n",
       "      <td>211</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32408</th>\n",
       "      <td>285</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105135</th>\n",
       "      <td>63</td>\n",
       "      <td>475</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83679</th>\n",
       "      <td>1703</td>\n",
       "      <td>229</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4385</th>\n",
       "      <td>2470</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71347</th>\n",
       "      <td>1103</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122159 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word_id  emoji_id  weight\n",
       "57744       492       100       1\n",
       "20885     10099        22       1\n",
       "41872      1671        53       1\n",
       "122006    14235      1297      10\n",
       "40583       211        52       1\n",
       "...         ...       ...     ...\n",
       "32408       285        37       1\n",
       "105135       63       475       6\n",
       "83679      1703       229       1\n",
       "4385       2470        11       1\n",
       "71347      1103       146       1\n",
       "\n",
       "[122159 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD, SVDpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply some baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5745512981715518\n",
      "0.5685175083164841\n",
      "0.5906072412476591\n",
      "0.5847890903573209\n",
      "0.5780746919874106\n"
     ]
    }
   ],
   "source": [
    "reader = Reader(rating_scale=(1,10))\n",
    "data = Dataset.load_from_df(rec_df[['word_id','emoji_id', 'weight']], reader)\n",
    "kf = KFold(n_splits=5)\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo = SVD(n_epochs=100)\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    print(accuracy.mae(predictions, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epo: 10\n",
      "2.2630787799430183\n",
      "2.2651218084006826\n",
      "2.2612007740459545\n",
      "2.274592767666333\n",
      "2.261736018412689\n",
      "Epo: 15\n",
      "2.2825234383166606\n",
      "2.2764753514928295\n",
      "2.283537704485021\n",
      "2.27949260525437\n",
      "2.2715511884832433\n",
      "Epo: 20\n",
      "2.274181809124232\n",
      "2.2808806787789355\n",
      "2.2923654921386243\n",
      "2.2876485218192966\n",
      "2.2960202385527633\n",
      "Epo: 25\n",
      "2.301118255934808\n",
      "2.2759921519447572\n",
      "2.284951253606249\n",
      "2.2807146902463473\n",
      "2.2906928433272644\n",
      "Epo: 30\n",
      "2.298290831049141\n",
      "2.2942903041132556\n",
      "2.2978235161687044\n",
      "2.290378376896937\n",
      "2.2947824177547065\n",
      "Epo: 35\n",
      "2.2910681098362287\n",
      "2.3091150218337777\n",
      "2.2960966714209263\n",
      "2.278933536585193\n",
      "2.287557945726545\n",
      "Epo: 40\n",
      "2.2940166958911785\n",
      "2.3050201295364308\n",
      "2.273115405773457\n",
      "2.2902336104180803\n",
      "2.281170923453311\n",
      "Epo: 45\n",
      "2.290335515581869\n",
      "2.28338474003143\n",
      "2.2988633363376794\n",
      "2.3123873115718054\n",
      "2.2870585872013\n"
     ]
    }
   ],
   "source": [
    "for Epo in range(10,50,5):\n",
    "    print(\"Epo:\",Epo)\n",
    "    for trainset, testset in kf.split(data):\n",
    "        algo = SVD(n_epochs=Epo)\n",
    "        algo.fit(trainset)\n",
    "        predictions = algo.test(testset)\n",
    "        print(accuracy.mae(predictions, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numFactor: 10\n",
      "2.2405843753853327\n",
      "2.250882760237451\n",
      "2.262324661871938\n",
      "2.271567048539495\n",
      "2.2481562677456046\n",
      "numFactor: 15\n",
      "2.2670949023262366\n",
      "2.2702554753364517\n",
      "2.28977385326955\n",
      "2.282471399683947\n",
      "2.254729505192972\n",
      "numFactor: 20\n",
      "2.27275800035319\n",
      "2.273506435220229\n",
      "2.2866123401318705\n",
      "2.2838360684101757\n",
      "2.271087853657984\n",
      "numFactor: 25\n",
      "2.2850697196442664\n",
      "2.3009130815719154\n",
      "2.2915283363203334\n",
      "2.290892377818273\n",
      "2.2718098393413126\n",
      "numFactor: 30\n",
      "2.2924794733903116\n",
      "2.2906999238394303\n",
      "2.282478069191084\n",
      "2.290597079643735\n",
      "2.27135095451869\n",
      "numFactor: 35\n",
      "2.299844785532678\n",
      "2.2966783860801785\n",
      "2.295739719654007\n",
      "2.2868723646159457\n",
      "2.293771588129018\n",
      "numFactor: 40\n",
      "2.2953382526492154\n",
      "2.29253525249451\n",
      "2.2937788799813954\n",
      "2.286710004334314\n",
      "2.2877780798679197\n",
      "numFactor: 45\n",
      "2.281992862927258\n",
      "2.3035781540948332\n",
      "2.2875192743150174\n",
      "2.296842255316335\n",
      "2.302946697681829\n"
     ]
    }
   ],
   "source": [
    "for numFactor in range(10,50,5):\n",
    "    print(\"numFactor:\",numFactor)\n",
    "    for trainset, testset in kf.split(data):\n",
    "        algo = SVD(n_factors=numFactor)\n",
    "        algo.fit(trainset)\n",
    "        predictions = algo.test(testset)\n",
    "        print(accuracy.mae(predictions, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3127181856928045\n",
      "2.2717430191206383\n",
      "2.27986205564029\n",
      "2.292383304419127\n",
      "2.288940078806011\n"
     ]
    }
   ],
   "source": [
    "rec_df = pd.read_csv(\"all_rec.csv\")\n",
    "rec_df = sklearn.utils.shuffle(rec_df,random_state=671)\n",
    "\n",
    "reader = Reader(rating_scale=(1,10))\n",
    "data = Dataset.load_from_df(rec_df[['word_id','emoji_id', 'weight']], reader)\n",
    "kf = KFold(n_splits=5)\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo = SVD(n_epochs=100)\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    print(accuracy.mae(predictions, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epo: 10\n",
      "2.281087685994986\n",
      "2.3025376343308217\n",
      "2.290967943155244\n",
      "2.2783210085760808\n",
      "2.295548425523583\n",
      "Epo: 15\n",
      "2.2939380218577323\n",
      "2.2911577603391464\n",
      "2.290860927082873\n",
      "2.3083703301077922\n",
      "2.3017557400735638\n",
      "Epo: 20\n",
      "2.3067662611009094\n",
      "2.3040988906518836\n",
      "2.285838474242703\n",
      "2.316827078843667\n",
      "2.2990869345964295\n",
      "Epo: 25\n",
      "2.3087526358343538\n",
      "2.310936674161544\n",
      "2.300216543656992\n",
      "2.288293790307453\n",
      "2.3041986727982398\n",
      "Epo: 30\n",
      "2.2878789349257675\n",
      "2.2928219303990938\n",
      "2.300355559125552\n",
      "2.304371802061929\n",
      "2.3092572342383715\n",
      "Epo: 35\n",
      "2.3035371526229094\n",
      "2.3067262635046837\n",
      "2.302119736045831\n",
      "2.2995035304564406\n",
      "2.2959834367807166\n",
      "Epo: 40\n",
      "2.2914909132190253\n",
      "2.297633659247429\n",
      "2.285864542951691\n",
      "2.300900814698275\n",
      "2.322125106003285\n",
      "Epo: 45\n",
      "2.2852900512750414\n",
      "2.309173931461023\n",
      "2.282335812005215\n",
      "2.3012337537899152\n",
      "2.2893188914107774\n"
     ]
    }
   ],
   "source": [
    "for Epo in range(10,50,5):\n",
    "    print(\"Epo:\",Epo)\n",
    "    for trainset, testset in kf.split(data):\n",
    "        algo = SVD(n_epochs=Epo)\n",
    "        algo.fit(trainset)\n",
    "        predictions = algo.test(testset)\n",
    "        print(accuracy.mae(predictions, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numFactor: 10\n",
      "2.283794949465569\n",
      "2.287377702669221\n",
      "2.2628856432505198\n",
      "2.2768887238368336\n",
      "2.2593797116608156\n",
      "numFactor: 15\n",
      "2.281471702587086\n",
      "2.278799396629668\n",
      "2.2769786733515898\n",
      "2.2792702286479893\n",
      "2.284990726699721\n",
      "numFactor: 20\n",
      "2.313926041818974\n",
      "2.285622521115598\n",
      "2.305580190719195\n",
      "2.3034166129794635\n",
      "2.267844616744328\n",
      "numFactor: 25\n",
      "2.290913135218191\n",
      "2.281968496785418\n",
      "2.3061673392597593\n",
      "2.305268560439139\n",
      "2.285147785965376\n",
      "numFactor: 30\n",
      "2.3105209351989697\n",
      "2.302026540016102\n",
      "2.2957071242488496\n",
      "2.299301253732101\n",
      "2.303591985557009\n",
      "numFactor: 35\n",
      "2.293022041615946\n",
      "2.3189443542549957\n",
      "2.28645040735304\n",
      "2.3069781485679512\n",
      "2.31364284592973\n",
      "numFactor: 40\n",
      "2.316022920531921\n",
      "2.3254813461166854\n",
      "2.302033619346231\n",
      "2.316766166173648\n",
      "2.3012633935651507\n",
      "numFactor: 45\n",
      "2.3178181633896155\n",
      "2.3073218279987517\n",
      "2.3078556107205745\n",
      "2.299126710391307\n",
      "2.3196950537472496\n"
     ]
    }
   ],
   "source": [
    "for numFactor in range(10,50,5):\n",
    "    print(\"numFactor:\",numFactor)\n",
    "    for trainset, testset in kf.split(data):\n",
    "        algo = SVD(n_factors=numFactor)\n",
    "        algo.fit(trainset)\n",
    "        predictions = algo.test(testset)\n",
    "        print(accuracy.mae(predictions, verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVDpp takes forever to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trainset, testset in kf.split(data):\n",
    "#     algo = SVDpp(n_epochs=100)\n",
    "#     algo.fit(trainset)\n",
    "#     predictions = algo.test(testset)\n",
    "#     print(accuracy.mae(predictions, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df.columns=[\"source\",\"target\",\"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57744</th>\n",
       "      <td>492</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20885</th>\n",
       "      <td>10099</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41872</th>\n",
       "      <td>1671</td>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122006</th>\n",
       "      <td>14235</td>\n",
       "      <td>1297</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40583</th>\n",
       "      <td>211</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32408</th>\n",
       "      <td>285</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105135</th>\n",
       "      <td>63</td>\n",
       "      <td>475</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83679</th>\n",
       "      <td>1703</td>\n",
       "      <td>229</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4385</th>\n",
       "      <td>2470</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71347</th>\n",
       "      <td>1103</td>\n",
       "      <td>146</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122159 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source  target  weight\n",
       "57744      492     100       2\n",
       "20885    10099      22       6\n",
       "41872     1671      53       5\n",
       "122006   14235    1297       3\n",
       "40583      211      52       6\n",
       "...        ...     ...     ...\n",
       "32408      285      37       4\n",
       "105135      63     475      10\n",
       "83679     1703     229       2\n",
       "4385      2470      11       7\n",
       "71347     1103     146       2\n",
       "\n",
       "[122159 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graphtype = nx.Graph()\n",
    "G = nx.from_pandas_edgelist(rec_df, edge_attr='weight', create_using=Graphtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.diameter(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.889818832420269"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.average_shortest_path_length(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.341056954755401"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.algorithms.cluster.average_clustering(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
